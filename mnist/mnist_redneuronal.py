# -*- coding: utf-8 -*-
"""MNIST_RedNeuronal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ye-nrSE2U1pkuT1Bu6WhbqrClTULWwT4
"""



# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf

from tensorflow import keras

import numpy as np

"""# Reconocimiento de caracteres con Redes Neuronales
Utilizando una red neuronal que será entrenada con el dataset MNIST.

70,000 imágenes de digitos escritos a mano.

60000 para entrenamiento.

10000 para prueba.
"""

mnist = tf.keras.datasets.mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

"""Las imágenes se encuentran codificadas como arrays Numpy

Representamos una imagen 28x28 para comprobar
"""

import matplotlib.pyplot as plt
plt.imshow(X_train[101], cmap=plt.cm.binary)
plt.xlabel("Etiquetado "+ str(y_train[101]))



print("Números de ejes: ", X_train.ndim)
print("Dimensiones: ", X_train.shape)
print("Tipo de datos: ", X_train.dtype)

"""60000 matrices de 28 x 28 que contienen enteros de 8 bits (0 - 255)

Escalado (normalización) de todos los valores al rango (0 - 1)
"""

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

"""Para facilitar la entrada a la red neuronal, se reduce el numpy array, en este caso de 2 dimensiones a una."""

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
print(X_train.shape)

"""
Para representar las etiquetas de cada imagen 0 - 9. Utilizamos un vector con 10 posiciones, colocando el valor 1 en la posición que corresponde a la etiqueta.
**Codificación One Hot Encoding**

"""

from tensorflow.keras.utils import to_categorical

print("Antes=====")
print(y_train[100], y_test[100], y_train.shape, y_test.shape)
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

print("Despues=====")
print(y_train[100], y_test[100], y_train.shape, y_test.shape)

"""# Definición del modelo
Como una secuencia de capas.
"""

model = keras.Sequential()
model.add(keras.layers.Dense(10, activation='sigmoid', input_shape=(784,)))
model.add(keras.layers.Dense(10, activation='softmax'))
model.summary()
# Un término de sesgo por neurona.

"""### Configurar el proceso de aprendizaje
* Función de coste, para evaluar el grado de error entre la salida calculada, y la salida deseada (etiquetas).
* La función optimizadora, es la que modifica los pesos.
* Métrica, medida para monitorizar el proceso de aprendizaje.


"""

# sgd gradiente descendente estocastico
# Categorical_crossentropy para dos o más etiquetas One Hot Encoding
# sparse_categorical_crossentropy categorias valores enteros.
model.compile(loss="categorical_crossentropy",
              optimizer="sgd",
              metrics=["accuracy"])

"""# Entrenamiento del modelo"""

history = model.fit(X_train, y_train, epochs=8)

"""Hay que observar la pérdida "loss", que es donde actua la función que hemos definido en la configuración del modelo, para realizar el ajuste de los pesos en cada una de las epochs.

Y la métrica elegida.
"""

import pandas as pd

history_df = pd.DataFrame(history.history)
# Start the plot at epoch 5. You can change this to get a different view.
history_df.loc[5:, ['loss']].plot();

"""# Evaluación del modelo"""



test_loss, test_acc = model.evaluate(X_test, y_test)

"""Basado en la matriz de confusión:

* La precisión (**accuracy** ) indica como de bien clasifica datos nuevos el modelo. \verdaderos negativos y positivos / total de predicciones
* La sensibilidad (**recall**) indica como de bien evita falsos negativos el modelo. verdaderos positivos / verdaderos positivos + falsos negativos
"""

print("Precisión Accuracy: ", test_acc)
print("Sensibilidad Recall: ", test_loss)

"""# Predicción
Mostramos una imagen cualquiera para luego ver la predicción.

Hay que realizar el reshape para que la imagen vuelva a un tamaño 28 x 28.
"""

predictions = model.predict(X_test)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

predicciones=[]
for i in predictions:
  predicciones.append(np.argmax(i))

reales=[]
for i in y_test:
    reales.append(np.argmax(i))


cm = confusion_matrix(reales, predicciones)

cm_display = ConfusionMatrixDisplay(cm).plot()

# La función de numpy argmax devuelve la posición del vector con valor más alto,
# es la posición de la clase que asigna.
posicion = 19
plt.imshow(X_test[posicion].reshape( 28, 28), cmap=plt.cm.binary)
plt.xlabel("Este número es un "+ str(np.argmax(predictions[posicion])))
print("Etiquetado como", np.argmax(y_test[posicion]))

"""La función softmax devuelve un vector de probabilidades de cada clase.

Y la suma de todas las probabilidades es 1
"""

print(predictions[posicion])
print("suma total: ", np.sum(predictions[posicion]))



